{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models (SVM, Logistic Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "\n",
    "[Requirements](#Requirements)   \n",
    "\n",
    "[SVM](#SVM)  \n",
    "[Model overview](#SVM-Model-overview)  \n",
    "[Proc and Cons](#SVM-Proc-and-Cons)  \n",
    "[Main params](#SVM-Main-params)  \n",
    "[Practice](#SVM-Practice)  \n",
    "[Useful links](#SVM-Useful-links)  \n",
    "[Task](#SVM-Task)  \n",
    "\n",
    "[Logistic Regression](#Logistic-Regression)  \n",
    "[Model overview](#LR-Model-overview)  \n",
    "[Proc and Cons](#LR-Proc-and-Cons)  \n",
    "[Main params](#LR-Main-params)  \n",
    "[Practice](#LR-Practice)  \n",
    "[Useful links](#LR-Useful-links)  \n",
    "[Task](#LR-Task)  \n",
    "\n",
    "[Logistic Regression vs SVM](#LR-vs-SVM)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "\n",
    "1. Python 3.x (or Anaconda3 for Python 3.5, https://www.continuum.io/downloads)\n",
    "2. Scikit-learn 0.18.x (pip install scikit-learn==0.18.1, http://scikit-learn.org/)\n",
    "3. NLTK lib latest (http://www.nltk.org/install.html)\n",
    "4. Pandas latest (http://pandas.pydata.org/)\n",
    "5. For datasets more than 1M reviews min Hardware Requirements (SDRAM >= 8 GB)\n",
    "\n",
    "[To the table of contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn.svm.LinearSVC  \n",
    "\n",
    "### SVM Model overview  \n",
    "\n",
    "\n",
    "A Support Vector Machine (SVM) is a supervised machine learning algorithm.\n",
    "\n",
    "SVMs are more commonly used in classification problems.\n",
    "\n",
    "SVMs are based on the idea of finding a hyperplane that best divides a dataset into two classes.\n",
    "\n",
    "<img src=\"../pictures/svm_intro.png\" alt=\"logistic\" style=\"width: 100%;\"/>\n",
    "\n",
    "\n",
    "** Support Vectors **\n",
    "\n",
    "Support vectors are the data points nearest to the hyperplane, the points of a data set that, if removed, would alter the position of the dividing hyperplane. Because of this, they can be considered the critical elements of a data set.\n",
    "What is a hyperplane?\n",
    "\n",
    " \n",
    "As a simple example, for a classification task with only two features (like the image above), you can think of a hyperplane as a line that linearly separates and classifies a set of data.\n",
    "\n",
    "Intuitively, the further from the hyperplane our data points lie, the more confident we are that they have been correctly classified. We therefore want our data points to be as far away from the hyperplane as possible, while still being on the correct side of it.\n",
    "\n",
    "So when new testing data is added, whatever side of the hyperplane it lands will decide the class that we assign to it.\n",
    "\n",
    "\n",
    "** How do we find the right hyperplane? **\n",
    "\n",
    "\n",
    "Or, in other words, how do we best segregate the two classes within the data?\n",
    "\n",
    "The distance between the hyperplane and the nearest data point from either set is known as the margin. The goal is to choose a hyperplane with the greatest possible margin between the hyperplane and any point within the training set, giving a greater chance of new data being classified correctly.\n",
    "\n",
    "<img src=\"../pictures/svm_margins.png\" alt=\"logistic\" style=\"width: 70%;\"/>\n",
    "\n",
    "\n",
    "** But what happens when there is no clear hyperplane? **  \n",
    "\n",
    "This is where it can get tricky. Data is rarely ever as clean as our simple example above.  \n",
    "A dataset will often look more like the jumbled balls below which represent a linearly non separable dataset.  \n",
    "In order to classify a dataset like the one above it’s necessary to move away from a 2d view of the data to a 3d view.  \n",
    "\n",
    "Imagine that our two sets of colored balls above are sitting on a sheet and this sheet is lifted suddenly, launching the balls into the air. While the balls are up in the air, you use the sheet to separate them. This ‘lifting’ of the balls represents the mapping of data into a higher dimension. This is known as kernelling. \n",
    "\n",
    "<img src=\"../pictures/svm_kerneling.png\" alt=\"logistic\" style=\"width: 70%;\"/>\n",
    "\n",
    "Because we are now in three dimensions, our hyperplane can no longer be a line. It must now be a plane as shown in the example above. The idea is that the data will continue to be mapped into higher and higher dimensions until a hyperplane can be formed to segregate it.\n",
    "\n",
    "\n",
    "** SVM Uses **\n",
    " \n",
    "SVM is used for text classification tasks such as category assignment, detecting spam and sentiment analysis.  \n",
    "It is also commonly used for image recognition challenges, performing particularly well in aspect-based recognition and color-based classification.  \n",
    "SVM also plays a vital role in many areas of handwritten digit recognition, such as postal automation services.\n",
    "\n",
    "\n",
    "[To the table of contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/you3liCbRZPrZA\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x7f056514fd30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('you3liCbRZPrZA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### SVM Proc and Cons\n",
    "\n",
    "** Pros **\n",
    "\n",
    "    + Simple\n",
    "    + Accuracy\n",
    "    + Works well on smaller cleaner datasets\n",
    "    + It can be more efficient because it uses a subset of training points\n",
    "\n",
    "\n",
    "** Cons **\n",
    "    - Isn’t suited to larger datasets as the training time with SVMs can be high (not for LinearSVC)\n",
    "    - Less effective on noisier datasets with overlapping classes\n",
    "\n",
    "\n",
    "\n",
    "[To the table of contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Main params\n",
    "\n",
    "** sklearn.svm.LinearSVC **\n",
    "\n",
    "** C ** : float, default: 1.0  \n",
    "    Inverse of regularization strength; must be a positive float.  \n",
    "    Like in support vector machines, smaller values specify stronger regularization. \n",
    "\n",
    "C: Penalty parameter C of the error term. It also controls the trade off between smooth decision boundary and classifying the training points correctly.\n",
    "    \n",
    "** loss ** : string, ‘hinge’ or ‘squared_hinge’ (default=’squared_hinge’)  \n",
    "    Specifies the loss function. ‘hinge’ is the standard SVM loss (used e.g. by the SVC class) while ‘squared_hinge’ is the square of the hinge loss.\n",
    "\n",
    "** penalty **: string, ‘l1’ or ‘l2’ (default=’l2’)  \n",
    "    Specifies the norm used in the penalization. The ‘l2’ penalty is the standard used in SVC. The ‘l1’ leads to coef_ vectors that are sparse.\n",
    "\n",
    "** tol **: float, optional (default=1e-4)\n",
    "    Tolerance for stopping criteria.s\n",
    "\n",
    "\n",
    "[To the table of contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import word_tokenize as nltk_wtknz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/movie_reviews.csv', sep = ',')\n",
    "train = train [1:10000]\n",
    "X = train.text\n",
    "Y = train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9999, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 152610 entries, 0 to 152609\n",
      "Data columns (total 2 columns):\n",
      "label    152610 non-null int64\n",
      "text     152610 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.587498\n",
       "0    0.412502\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.describe()\n",
    "# df.describe(include=['object'])\n",
    "# df['label'].value_counts()\n",
    "train['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    word_list = nltk_wtknz(text)\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stems = [stemmer.stem(word) for word in word_list]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline_svm = Pipeline([('vectorizer', TfidfVectorizer(ngram_range=(1, 3))),\n",
    "                     ('clf_svm', LinearSVC(class_weight='balanced'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('clf_svm',\n",
       " LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\n",
       "      intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "      multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "      verbose=0))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_svm.steps[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 57s, sys: 590 ms, total: 1min 58s\n",
      "Wall time: 1min 58s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=Tr...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'clf_svm__C': [100, 10, 1]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_svm = dict(clf_svm__C=[100, 10, 1])\n",
    "grid_search_svm = GridSearchCV(pipeline_svm, param_grid=params_svm, cv=5, scoring='accuracy')\n",
    "%time grid_search_svm.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'clf_svm__C': 10}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      6537\n",
      "          1       1.00      1.00      1.00     13462\n",
      "\n",
      "avg / total       1.00      1.00      1.00     19999\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"# Tuning hyper-parameters\")\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(grid_search_svm.best_params_)\n",
    "print()\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "print(\"The model is trained on the full development set.\")\n",
    "print(\"The scores are computed on the full evaluation set.\")\n",
    "print()\n",
    "y_true, y_pred = Y, grid_search_svm.predict(X)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 32s, sys: 1.37 s, total: 2min 34s\n",
      "Wall time: 2min 34s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=Tr...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_svm = Pipeline([('vectorizer', TfidfVectorizer(ngram_range=(1, 2))),\n",
    "                     ('clf_svm', LinearSVC(class_weight='balanced', C = 10))])\n",
    "%time pipeline_svm.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     62952\n",
      "          1       1.00      1.00      1.00     89658\n",
      "\n",
      "avg / total       1.00      1.00      1.00    152610\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = Y, pipeline_svm.predict(X)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('../dumps/m_lin_svc_mix_out2.pkl', 'wb') as f:\n",
    "#     pickle.dump(pipeline, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/test.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10660, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>it's so laddish and juvenile , only teenage bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>exploitative and largely devoid of the depth o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[garbus] discards the potential for pathologic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>a visually flashy but narratively opaque and e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>the story is also as unoriginal as they come ,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  it's so laddish and juvenile , only teenage bo...\n",
       "1      0  exploitative and largely devoid of the depth o...\n",
       "2      0  [garbus] discards the potential for pathologic...\n",
       "3      0  a visually flashy but narratively opaque and e...\n",
       "4      0  the story is also as unoriginal as they come ,..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('../dumps/m_lin_svc_mix_out.pkl', 'rb') as f:\n",
    "#     model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted_svm = pipeline_svm.predict(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.839212007505\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.79      0.83      5330\n",
      "          1       0.81      0.89      0.85      5330\n",
      "\n",
      "avg / total       0.84      0.84      0.84     10660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(df['label'], y_predicted_svm))\n",
    "print(classification_report(df['label'], y_predicted_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Useful links\n",
    "\n",
    "http://cs229.stanford.edu/materials.html  \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html  \n",
    "https://www.csie.ntu.edu.tw/~cjlin/liblinear/  \n",
    "http://www.kdnuggets.com/2016/07/support-vector-machines-simple-explanation.html  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Task\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[To the table of contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Train SVM model with different paprameters/preprocessing\n",
    "### Check accuracy on test set.\n",
    "\n",
    "#### Your code here###\n",
    "\n",
    "#my_pipeline_svm = pipeline()\n",
    "\n",
    "#### Your Code Ends Here\n",
    "\n",
    "#my_pipeline_svm.fit(X, Y)\n",
    "#y_predicted_svm = my_pipeline_svm.predict(df['text'])\n",
    "#print(accuracy_score(df['label'], y_predicted_lr))\n",
    "#print(classification_report(df['label'], y_predicted_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Model overview  \n",
    "\n",
    "Logistic regression is a linear model for classification rather than regression.  \n",
    "Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier.  \n",
    "In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function (sigmoid function).  \n",
    "\n",
    "<img src=\"../pictures/sphx_glr_plot_logistic_001.png\" alt=\"logistic\" style=\"width: 70%;\"/>\n",
    "\n",
    "Decision boundary “separates” variable space into two decision regions.\n",
    "Linear regression not advised for classification. First, it is sensitive to outliers/skewed data sets.\n",
    "\n",
    "We will focus on ** sklearn.linear_model.LogisticRegression ** (liblinear)\n",
    "This class implements regularized logistic regression using the \"liblinear\" library.\n",
    "\n",
    "\n",
    "[To the table of contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Proc and Cons\n",
    "\n",
    "** Pros **\n",
    "\n",
    "    + simple\n",
    "    + good scaling for huge data\n",
    "    + fast\n",
    "\n",
    "** Cons **\n",
    "    - not for non-linear data\n",
    "\n",
    "[To the table of contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic function and SoftMax\n",
    "\n",
    "\n",
    "### Logistic function\n",
    "$$  \n",
    "    F(x) = \\frac{1}{1+e^{-(w_0 + x^T * w_1)}} \n",
    "$$\n",
    "\n",
    "<br> where $w_0$ corresponds to intercept of the model, \n",
    "<br>$w_1$ - vector of model coefficients, \n",
    "<br>$x$ - feature vector \n",
    "\n",
    "\n",
    "### SoftMax\n",
    "$$  \n",
    "    F(x) = \\frac{e^{ x^T * w_j}}{\\sum_{k=1} e^{ x^T * w_k}} \n",
    "$$\n",
    "\n",
    "<br> where $w_j$ model weights corresponding to class j,  \n",
    "<br>$x$ - feature vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Main params\n",
    "\n",
    "** C ** : float, default: 1.0  \n",
    "    Inverse of regularization strength; must be a positive float.  \n",
    "    Like in support vector machines, smaller values specify stronger regularization.  \n",
    "\n",
    "** class_weight **: dict or ‘balanced’, default: None\n",
    "    Weights associated with classes in the form {class_label: weight}.  \n",
    "    If not given, all classes are supposed to have weight one.\n",
    "\n",
    "** solver **: {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’}, default: ‘liblinear’\n",
    "    Algorithm to use in the optimization problem.\n",
    "        For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ is\n",
    "            faster for large ones.\n",
    "        For multiclass problems, only ‘newton-cg’, ‘sag’ and ‘lbfgs’ handle\n",
    "            multinomial loss; ‘liblinear’ is limited to one-versus-rest schemes.\n",
    "\n",
    "** tol **: float, default: 1e-4\n",
    "    Tolerance for stopping criteria.\n",
    "\n",
    "\n",
    "** n_jobs ** : int, default: 1\n",
    "    Number of CPU cores used during the cross-validation loop. If given a value of -1, all cores are used.\n",
    "\n",
    "\n",
    "[To the table of contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline_lr = Pipeline([('vectorizer', TfidfVectorizer(ngram_range=(1, 2))),\n",
    "                     ('clf_lr', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('clf_lr',\n",
       " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_lr.steps[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_lr = dict(clf_lr__C=[0.01, 0.1, 1, 10])\n",
    "grid_search_lr = GridSearchCV(pipeline_lr, param_grid=params_lr, cv=5, scoring='accuracy')\n",
    "%time grid_search_lr.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"# Tuning hyper-parameters\")\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(grid_search_lr.best_params_)\n",
    "print()\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "print(\"The model is trained on the full development set.\")\n",
    "print(\"The scores are computed on the full evaluation set.\")\n",
    "print()\n",
    "y_true, y_pred = Y, grid_search_lr.predict(X)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=Tr...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_lr = Pipeline([('vectorizer', TfidfVectorizer(ngram_range=(1, 2))),\n",
    "                     ('clf_lr', LogisticRegression(C = 1))])\n",
    "pipeline_lr.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.809099437148\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.71      0.79      5330\n",
      "          1       0.76      0.91      0.83      5330\n",
      "\n",
      "avg / total       0.82      0.81      0.81     10660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predicted_lr = pipeline_lr.predict(df['text'])\n",
    "print(accuracy_score(df['label'], y_predicted_lr))\n",
    "print(classification_report(df['label'], y_predicted_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Useful links\n",
    "\n",
    "http://cs229.stanford.edu/materials.html  \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html  \n",
    "https://habrahabr.ru/company/ods/blog/323890/  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Task\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[To the table of contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Train Logistic Regression model with different paprameters/preprocessing/ \n",
    "### Check accuracy on test set.\n",
    "\n",
    "\n",
    "\n",
    "#### Your code here###\n",
    "\n",
    "#my_pipeline_lr = pipeline()\n",
    "\n",
    "#### Your Code Ends Here\n",
    "\n",
    "#my_pipeline_lr.fit(X, Y)\n",
    "#y_predicted_lr = my_pipeline_lr.predict(df['text'])\n",
    "#print(accuracy_score(df['label'], y_predicted_lr))\n",
    "#print(classification_report(df['label'], y_predicted_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR vs SVM  \n",
    "\n",
    "In practical classification tasks, linear logistic regression and linear SVMs often yield very similar results.  \n",
    "** Logistic regression ** tries to maximize the conditional likelihoods of the training data, which makes it more prone to outliers than SVMs.  \n",
    "** The SVMs ** mostly care about the points that are closest to the decision boundary (support vectors). \n",
    "\n",
    "On the other hand, logistic regression has the advantage that it is a simpler model that can be implemented more easily.   Furthermore, logistic regression models can be easily updated, which is attractive when working with streaming data.\n",
    "\n",
    "\n",
    "[To the table of contents](#Table-of-Contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
