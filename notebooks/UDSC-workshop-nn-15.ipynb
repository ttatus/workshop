{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks in 15 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neurons and Computations Graphs\n",
    "\n",
    "http://bit.ly/2ppOEKG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "$\\sigma(\\mathbf{z})_j = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}$\n",
    "\n",
    "http://neuralnetworksanddeeplearning.com/chap3.html#softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossentropy\n",
    "\n",
    "$H(p, q) = -\\sum_x p(x)\\, \\log q(x).$\n",
    "\n",
    "### Binary Crossentropy\n",
    "\n",
    "$H(p,q)\\ =\\ -\\sum_ip_i\\log q_i\\ =\\ -y\\log\\hat{y} - (1-y)\\log(1-\\hat{y})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline\n",
    "# with this instead of the above\n",
    "# %matplotlib notebook\n",
    "# you can rotate plot interactively\n",
    "\n",
    "# # Make data.\n",
    "\n",
    "# eps = 1e-2\n",
    "\n",
    "# Y = np.linspace(eps,1-eps,100)\n",
    "# Y_hat = np.linspace(eps,1-eps,100)\n",
    "\n",
    "# Y, Y_hat = np.meshgrid(Y, Y_hat)\n",
    "\n",
    "# Xent = - Y * np.log(Y_hat) - (1-Y)*np.log(1-Y_hat)\n",
    "# Xent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "# ax = fig.gca(projection='3d')\n",
    "\n",
    "# # Plot the surface.\n",
    "# surf = ax.plot_surface(Y, Y_hat, Xent, cmap=plt.cm.coolwarm,\n",
    "#                        linewidth=0, antialiased=False)\n",
    "\n",
    "# ax.view_init(30, 225)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that 0 and 1 are actually bad!\n",
    "\n",
    "We don't encounter it in practice, because sigmoid never goes to 1 or 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST task with simple feed-forward network\n",
    "\n",
    "We want to have is a function F(image) -> class\n",
    "\n",
    "Image is a vector of dim 784\n",
    "\n",
    "We will approximy it with\n",
    "\n",
    "\\begin{align}\n",
    "F(X) = softmax(X*W + b)\n",
    "\\end{align}\n",
    "\n",
    "Which is, actually, a multinomial logistic regression\n",
    "\n",
    "We will minimize loss function of crossentropy using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# DATA_DIR = \"/tmp/tensorflow/mnist/input_data\"\n",
    "# mnist = input_data.read_data_sets(DATA_DIR, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, labels = mnist.train.next_batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # let's somehow peek into this data\n",
    "\n",
    "# (images.shape,\n",
    "# images[0][300:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show one image\n",
    "\n",
    "# plt.imshow(images[2].reshape((28,28)), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show many\n",
    "\n",
    "# fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "# for i in range(len(images)):\n",
    "#     a=fig.add_subplot(4,4,i+1,xticks=[], yticks=[])\n",
    "#     imgplot = plt.imshow(images[i].reshape((28,28)), cmap='Greys')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.tensorflow.org/versions/r0.10/images/softmax-regression-vectorequation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this might be be numerically unstable on some edge-case inputs:\n",
    "# \n",
    "# tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.nn.softmax(y)),\n",
    "#                                 reduction_indices=[1]))\n",
    "#\n",
    "# so, there is a special function for this:\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "  tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grads = optimizer.compute_gradients(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_params = optimizer.apply_gradients(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize training ops and session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize tensorflow state\n",
    "\n",
    "# # should be \n",
    "# # tf.global_variable_initializer().run() for tf 0.11 and later\n",
    "# sess = tf.InteractiveSession()\n",
    "\n",
    "# tf.initialize_all_variables().run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_step(batch_size):\n",
    "#     batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "#     sess.run(update_params, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W1, b1 = sess.run((W, b))\n",
    "# W1.shape, b1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quick peek into gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_xs, batch_ys = mnist.train.next_batch(1)\n",
    "# g0 = sess.run(grads, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g0[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in range(0):\n",
    "#     train_step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get and draw contents of W matrix\n",
    "\n",
    "# W1 = sess.run(W)\n",
    "\n",
    "# fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "# for i in range(10):\n",
    "#     P = fig.add_subplot(2,5,i+1, xticks=[], yticks=[])\n",
    "#     P.set_title(i, fontsize=30)\n",
    "#     imgplot = plt.imshow(W1[:,i].reshape((28,28)), cmap=plt.cm.RdGy_r)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Verify accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# print(sess.run(accuracy, feed_dict={x: mnist.test.images,\n",
    "#                                     y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look into direct output before softmax\n",
    "\n",
    "# logits = np.matmul(images[0], W1) + b1\n",
    "\n",
    "# plt.bar(np.arange(10), logits)\n",
    "# plt.xticks(range(10))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = sess.run(tf.nn.softmax(np.matmul(images[0], W1) + b1))\n",
    "\n",
    "# plt.bar(np.arange(10), p)\n",
    "# plt.xticks(range(10))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And Deeper!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/nn15-kolmogorov-deeper.png)\n",
    "\n",
    "http://www.ccas.ru/voron/download/NeuralNets.pdf\n",
    "\n",
    "Лекции по искусственным нейронным сетям К. В. Воронцов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNs can separate data with complex lines:\n",
    "\n",
    "![](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/img/simple2_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The other way to look at this, is that they warp space, and then apply linear separation:\n",
    "\n",
    "![](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/img/simple2_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips and tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate\n",
    "\n",
    "![](img/nn15-learningrage.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  What if weights explode?\n",
    "\n",
    "Why would they?\n",
    "\n",
    "Add penalty to loss function, and optimize towards lesser weights explicitly, i.e. L2\n",
    "\n",
    "$H(p, q) = -\\sum_x p(x)\\, \\log q(x) + L2(W).$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfit?\n",
    "\n",
    "### Add dropout!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1502.03167\n",
    "\n",
    "Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin\n",
    "\n",
    "![](img/nn15-sigmoid.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug with tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=eBbEDRsCmv4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
